{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations\n",
    "transform = A.Compose([\n",
    "    A.Resize(256, 256),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "], is_check_shapes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset\n",
    "class FloodDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, csv_file, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.metadata = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.image_dir, self.metadata.iloc[idx, 0])\n",
    "        mask_name = os.path.join(self.mask_dir, self.metadata.iloc[idx, 1])\n",
    "        \n",
    "        image = np.array(Image.open(img_name).convert(\"RGB\"))\n",
    "        mask = np.array(Image.open(mask_name).convert(\"L\"))\n",
    "        \n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "        \n",
    "        mask = mask / 255.0  # Normalize mask to [0, 1]\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader\n",
    "image_dir = 'Image'\n",
    "mask_dir = 'Mask'\n",
    "csv_file = 'metadata.csv'\n",
    "\n",
    "dataset = FloodDataset(image_dir, mask_dir, csv_file, transform)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class SegmentationModel(nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super(SegmentationModel, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        self.classifier = nn.Conv2d(backbone._fc.in_features, 1, kernel_size=1)\n",
    "        self.upsample = nn.Upsample(scale_factor=32, mode='bilinear', align_corners=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone.extract_features(x)\n",
    "        out = self.classifier(features)\n",
    "        out = self.upsample(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b3\n"
     ]
    }
   ],
   "source": [
    "# Using a pretrained EfficientNet-B7 model\n",
    "backbone = EfficientNet.from_pretrained('efficientnet-b3')\n",
    "model = SegmentationModel(backbone)\n",
    "\n",
    "# Training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)  # Reduced learning rate\n",
    "\n",
    "num_epochs = 20\n",
    "checkpoint_dir = './checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(outputs, masks):\n",
    "    outputs = torch.sigmoid(outputs)  # Apply sigmoid to get probabilities\n",
    "    preds = outputs > 0.5  # Convert probabilities to binary predictions\n",
    "    correct = (preds == masks).float()  # Compare predictions with masks\n",
    "    accuracy = correct.sum() / correct.numel()  # Calculate accuracy\n",
    "    return accuracy.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(epoch, model, optimizer, loss, checkpoint_dir):\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch}.pth')\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "    }, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.37846991097604904, Accuracy: 0.8196677646121463\n",
      "Epoch 2/20, Loss: 0.3713189733994974, Accuracy: 0.8244869644577438\n",
      "Epoch 3/20, Loss: 0.3581811165487444, Accuracy: 0.831235782520191\n",
      "Epoch 4/20, Loss: 0.34837783268980077, Accuracy: 0.835702844568201\n",
      "Epoch 5/20, Loss: 0.3470550018387872, Accuracy: 0.8347592224945893\n",
      "Epoch 6/20, Loss: 0.33909591146417567, Accuracy: 0.839314434979413\n",
      "Epoch 7/20, Loss: 0.33336026120830226, Accuracy: 0.8420373040276605\n",
      "Epoch 8/20, Loss: 0.32428288620871465, Accuracy: 0.8473150923445418\n",
      "Epoch 9/20, Loss: 0.3239408739515253, Accuracy: 0.8468552666741449\n",
      "Epoch 10/20, Loss: 0.3166873261735246, Accuracy: 0.8489612888645481\n",
      "Epoch 11/20, Loss: 0.3112920417978957, Accuracy: 0.8515611081509977\n",
      "Epoch 12/20, Loss: 0.3161034950533429, Accuracy: 0.8503168982428473\n",
      "Epoch 13/20, Loss: 0.3103532033997613, Accuracy: 0.8525498364422772\n",
      "Epoch 14/20, Loss: 0.3206770375773713, Accuracy: 0.8489866514463682\n",
      "Epoch 15/20, Loss: 0.30624027751587535, Accuracy: 0.8549740250046188\n",
      "Epoch 16/20, Loss: 0.29787652315320196, Accuracy: 0.8594420149519637\n",
      "Epoch 17/20, Loss: 0.30859190507515055, Accuracy: 0.8548028275773332\n",
      "Epoch 18/20, Loss: 0.3006037147463979, Accuracy: 0.8586479908711201\n",
      "Epoch 19/20, Loss: 0.297675987353196, Accuracy: 0.8588937810949377\n",
      "Epoch 20/20, Loss: 0.2930047367070172, Accuracy: 0.8599659687763935\n",
      "Training Complete!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_accuracy = 0.0\n",
    "    \n",
    "    for images, masks in dataloader:\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device).unsqueeze(1).float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        running_accuracy += calculate_accuracy(outputs, masks)\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_accuracy = running_accuracy / len(dataloader)\n",
    "    print(f\"Epoch {epoch}/{num_epochs}, Loss: {epoch_loss}, Accuracy: {epoch_accuracy}\")\n",
    "    \n",
    "    # Save checkpoint every 5 epochs\n",
    "    if epoch % 5 == 0:\n",
    "        save_checkpoint(epoch, model, optimizer, epoch_loss, checkpoint_dir)\n",
    "        # Ask user if they want to continue\n",
    "        continue_training = input(f\"Training reached epoch {epoch}. Do you want to continue? (yes/no): \")\n",
    "        if continue_training.lower() != 'yes':\n",
    "            print(\"Training stopped by user.\")\n",
    "            break\n",
    "\n",
    "print(\"Training Complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
