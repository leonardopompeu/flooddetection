{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.models as models\n",
    "from torchvision.transforms import ToTensor\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset\n",
    "class FloodDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, csv_file, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.metadata = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.image_dir, self.metadata.iloc[idx, 0])\n",
    "        mask_name = os.path.join(self.mask_dir, self.metadata.iloc[idx, 1])\n",
    "        \n",
    "        image = np.array(Image.open(img_name).convert(\"RGB\"))\n",
    "        mask = np.array(Image.open(mask_name).convert(\"L\"))\n",
    "        \n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "        \n",
    "        mask = mask / 255.0  # Normalize mask to [0, 1]\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations\n",
    "transform = A.Compose([\n",
    "    A.Resize(256, 256),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "], is_check_shapes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data Loader\n",
    "image_dir = 'Image'\n",
    "mask_dir = 'Mask'\n",
    "csv_file = 'metadata.csv'\n",
    "\n",
    "\n",
    "dataset = FloodDataset(image_dir, mask_dir, csv_file, transform)\n",
    "dataloader = DataLoader(dataset, batch_size=12, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class SegmentationModel(nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super(SegmentationModel, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        self.classifier = nn.Conv2d(2048, 1, kernel_size=1)\n",
    "        self.upsample = nn.Upsample(scale_factor=32, mode='bilinear', align_corners=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        out = self.classifier(features)\n",
    "        out = self.upsample(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adtsys/anaconda3/envs/detection/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/adtsys/anaconda3/envs/detection/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Using a pretrained ResNet model\n",
    "backbone = models.resnet50(pretrained=True)\n",
    "backbone = nn.Sequential(*list(backbone.children())[:-2])\n",
    "model = SegmentationModel(backbone)\n",
    "\n",
    "# Training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)  # Reduced learning rate\n",
    "\n",
    "num_epochs = 20\n",
    "checkpoint_dir = './checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(epoch, model, optimizer, loss, checkpoint_dir):\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch}.pth')\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "    }, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_accuracy(outputs, masks):\n",
    "    outputs = torch.sigmoid(outputs)  # Apply sigmoid to get probabilities\n",
    "    preds = outputs > 0.5  # Convert probabilities to binary predictions\n",
    "    correct = (preds == masks).float()  # Compare predictions with masks\n",
    "    accuracy = correct.sum() / correct.numel()  # Calculate accuracy\n",
    "    return accuracy.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.6595405101776123, Accuracy: 0.5865057361125946\n",
      "Epoch 2/20, Loss: 0.5448558747768402, Accuracy: 0.7463970446586609\n",
      "Epoch 3/20, Loss: 0.4876483500003815, Accuracy: 0.7749938464164734\n",
      "Epoch 4/20, Loss: 0.47275407195091246, Accuracy: 0.7807779431343078\n",
      "Epoch 5/20, Loss: 0.44229947686195376, Accuracy: 0.7927289295196533\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "save_checkpoint() missing 1 required positional argument: 'checkpoint_dir'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Save checkpoint every 5 epochs\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 25\u001b[0m     \u001b[43msave_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch_loss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# Ask user if they want to continue\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining reached epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Do you want to continue? (yes/no): \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: save_checkpoint() missing 1 required positional argument: 'checkpoint_dir'"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_accuracy = 0.0\n",
    "    \n",
    "    for images, masks in dataloader:\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device).unsqueeze(1).float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        running_accuracy += calculate_accuracy(outputs, masks)\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_accuracy = running_accuracy / len(dataloader)\n",
    "    print(f\"Epoch {epoch}/{num_epochs}, Loss: {epoch_loss}, Accuracy: {epoch_accuracy}\")\n",
    "    \n",
    "    # Save checkpoint every 5 epochs\n",
    "    if epoch % 5 == 0:\n",
    "        save_checkpoint(epoch, model, optimizer, epoch_loss, checkpoint_dir)\n",
    "        # Ask user if they want to continue\n",
    "        continue_training = input(f\"Training reached epoch {epoch}. Do you want to continue? (yes/no): \")\n",
    "        if continue_training.lower() != 'yes':\n",
    "            print(\"Training stopped by user.\")\n",
    "            break\n",
    "\n",
    "print(\"Training Complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
